{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. torchtext 설치 및 hyperparameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torchtext==0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data, datasets\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b5c17ee7b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 5\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size(:하나의 batch 내에 들어있는 문장 개수),  문장 내 단어 개수]\n",
    "# [enforce fail at cpuallocator.cpp:68] . defaultCPUallocator: can't allocate memory: you tried to allocate 8247153600 bytes. error code 12 (cannot allocate memory)\n",
    "# batch_size = 256 으로 설정했을 때 발생.\n",
    "# CPU가 감당 못하는 너무 큰 메모리를 할당하려고 할 때 발생하는 에러 -> 실제로 학습시간이 1시간을 넘어갔다.\n",
    "# batch_size = 64 로 변경\n",
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# CUDA(Compute Unified Device Architecture)\n",
    "# GPU의 가상 명령어셋을 사용할 수 있도록 만들어주는 소프트웨어 레이어로, \n",
    "# NVIDIA가 만든 CUDA 코어가 장착된 GPU에서 작동 Many-Core dependent 연산\n",
    "# 많은 양의 연산을 동시에 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. torchtext.data.Field 에 따른 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "TEXT = data.Field(sequential=True, use_vocab=True, batch_first=True,lower=True)\n",
    "LABEL = data.Field(sequential=false, use_vocab=True, batch_first=True)\n",
    "\n",
    "# Distribute the Train / Test data in 1:1\n",
    "train_dataset, test_dataset = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "print('Print the elements of train_set : ', train_dataset.fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Generate Vocabulary Dictionary and set additional parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_dataset, min_freq=5)\n",
    "LABEL.build_vocab(train_dataset)\n",
    "# label 에 해당하는 값이 정수가 아니라 'pos', 'neg' 와 같은 string 이기 때문에\n",
    "# 이를 단어 사전 내의 정수 index 로 치환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언에서 사용할 parameter\n",
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2\n",
    "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
    "print('클래스의 개수 : {}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 / 검증 데이터 8:2 분리, label 비율 유지\n",
    "train_set, validation_set = train_dataset.split(split_ratio=0.8, stratified=True, strata_field='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transmit to Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = Iterator.splits((train_set, validation_set, test_dataset), \n",
    "                                                  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print('Number of mini-batch for train data : {}'.format(len(train_iter)))\n",
    "print('Number of mini-batch for test data : {}'.format(len(test_iter)))\n",
    "print('Number of mini-batch for validation data : {}'.format(len(val_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(batch.text.shape)\n",
    "print()\n",
    "print(batch.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "batch.text.shape() # MAX_LENGTH 에 따른 padding 처리가 안됨\n",
    "# batch 안의 가장 긴 sentence의 길이가 MAX_LENGTH가 되어 padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = batch.label\n",
    "print(y)\n",
    "y.sub_(1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.text.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = Iterator.splits((train_set, validation_set, test_dataset), \n",
    "                                                  batch_size=BATCH_SIZE, shuffle=True)\n",
    "# 앞서 next 했던 값을 다시 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rnn_img/image.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self,n_layers,hidden_dim,n_vocab,embed_dim,n_class,dropout_p=0.2):\n",
    "        super(GRU,self).__init__()\n",
    "        self.n_layers=n_layers\n",
    "        self.hidden_dim=hidden_dim\n",
    "        \n",
    "        self.embed=nn.Embedding(n_vocab,embed_dim)\n",
    "        self.dropout=nn.Dropout(dropout_p)\n",
    "        self.gru=nn.GRU(embed_dim,self.hidden_dim,num_layers=self.n_layers,batch_first=True)\n",
    "        self.out=nn.Linear(self.hidden_dim,n_class)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.embed(x) # Embedding\n",
    "        x,_=self.gru(x) # GRU\n",
    "        last_h=x[:,-1,:] # extract last hidden state vector per sentence\n",
    "        self.dropout(last_h) # Dropout\n",
    "        logit=self.out(last_h) # Linear\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Model Compling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU(1, 64, vocab_size, 128, n_classes, 0.5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum').to(device)\n",
    "# 하나의 batch 에 대한 loss 값을 모두 더한 것을 그 batch 의 total_loss로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,criterion,train_iter):\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(device), batch.label.to(device)\n",
    "        \n",
    "        # label의 값들 1,2 에서 1을 빼서 0,1로 변환\n",
    "        # label_vocab : {'<unk>':0, 'neg':1, 'pos':2}\n",
    "        # y 와 y.data는 동일 -> y.data.sub_(1) == y.sub_(1)\n",
    "        y.data.sub_(1)\n",
    "        \n",
    "        logit = model(x)\n",
    "        loss = criterion(logit, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Functionalization for train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,val_iter,criterion):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for batch in val_iter:\n",
    "        x, y = batch.text.to(device), batch.label.to(device)\n",
    "        y.data.sub_(1)\n",
    "        logit = model(x)\n",
    "        loss = criterion(logit, y)\n",
    "        total_loss += loss\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    \n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rnn_img/image (1).png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1,0],[1,0],[0,1],[1,0]])\n",
    "a.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([0,0,0,1])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.max(1)[1] == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a.max(1)[1] == y).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rnn_img/image (2).png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = None # 최고 성능을 낸 검증 loss 값을 저장\n",
    "\n",
    "for e in range(1, EPOCHS+1):\n",
    "    train(model,optimizer,criterion,train_iter)\n",
    "    val_loss, val_accuracy = evaluate(model,val_iter,criterion)\n",
    "    \n",
    "    print(\"Epoch : {} | Val_loss : {:5.2f}, Val_accuracy : {:5.2f}\".format(e,val_loss,val_accuracy))\n",
    "    \n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        torch.save(model.state_dict(),'./snapshot/txtclassification.pt') # 가장 성능 좋은 가중치 저장\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./snapshot/txtclassificaiton.pt'))\n",
    "test_loss,test_acc=evaluate(model,test_iter,criterion)\n",
    "print('테스트 오차: {:5.2f} | 테스트 정확도: {:5.2f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rnn_img/Untitled.png\"/>\n",
    "<img src=\"./rnn_img/Untitled_(1).png\"/>\n",
    "<img src=\"./rnn_img/Untitled_(2).png\"/>\n",
    "<img src=\"./rnn_img/Untitled_(3).png\"/>\n",
    "<img src=\"./rnn_img/Untitled_(4).png\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9271e414be5e055cabef0148537efe95905a2cbc3a51060d18455594802bc000"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
