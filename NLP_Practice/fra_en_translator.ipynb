{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Franch to Engish Machine Translator\n",
    "\n",
    "#### 병렬 corpus 데이터에 대한 이해와 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement shutil (from versions: none)\n",
      "ERROR: No matching distribution found for shutil\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# %pip install urllib3\n",
    "# %pip install zipfile\n",
    "# %pip install shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib3\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file downloaded to fra-eng.zip\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def download_zip(url, output_path):\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"ZIP file downloaded to {output_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download. HTTP Response Code: {response.status_code}\")\n",
    "\n",
    "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "output_path = \"fra-eng.zip\"\n",
    "download_zip(url, output_path)\n",
    "\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, output_path)\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수 : 229803\n"
     ]
    }
   ],
   "source": [
    "lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "print('전체 샘플의 개수 :',len(lines))\n",
    "# src : Source = 번역 대상, tar : Target = 번역 결과;Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36264</th>\n",
       "      <td>The plate is dirty.</td>\n",
       "      <td>L'assiette est sale.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35339</th>\n",
       "      <td>Let's hit the road.</td>\n",
       "      <td>Cassons-nous !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54619</th>\n",
       "      <td>Are these your things?</td>\n",
       "      <td>Ces choses sont-elles à vous ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32272</th>\n",
       "      <td>Does that mean yes?</td>\n",
       "      <td>Cela signifie-t-il oui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44030</th>\n",
       "      <td>This flight is full.</td>\n",
       "      <td>Ce vol est complet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47669</th>\n",
       "      <td>He missed his flight.</td>\n",
       "      <td>Il a loupé son vol.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50559</th>\n",
       "      <td>It's our only chance.</td>\n",
       "      <td>C'est la seule chance que nous ayons.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27437</th>\n",
       "      <td>I was unconscious.</td>\n",
       "      <td>J’ai perdu connaissance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21478</th>\n",
       "      <td>I'm feeling sick.</td>\n",
       "      <td>Je me sens malade.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12093</th>\n",
       "      <td>Is that coffee?</td>\n",
       "      <td>Est-ce que c'est du café ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                                    tar\n",
       "36264     The plate is dirty.                   L'assiette est sale.\n",
       "35339     Let's hit the road.                         Cassons-nous !\n",
       "54619  Are these your things?         Ces choses sont-elles à vous ?\n",
       "32272     Does that mean yes?               Cela signifie-t-il oui ?\n",
       "44030    This flight is full.                    Ce vol est complet.\n",
       "47669   He missed his flight.                    Il a loupé son vol.\n",
       "50559   It's our only chance.  C'est la seule chance que nous ayons.\n",
       "27437      I was unconscious.               J’ai perdu connaissance.\n",
       "21478       I'm feeling sick.                     Je me sens malade.\n",
       "12093         Is that coffee?             Est-ce que c'est du café ?"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[0:60000] # 6만개만 저장\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46333</th>\n",
       "      <td>A squid has ten legs.</td>\n",
       "      <td>\\t Un calamar est muni de dix pattes. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9245</th>\n",
       "      <td>That's my dad.</td>\n",
       "      <td>\\t C'est mon papa. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40615</th>\n",
       "      <td>I don't have a clue.</td>\n",
       "      <td>\\t Je n'en ai pas la moindre idée. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41639</th>\n",
       "      <td>I won't tell anyone.</td>\n",
       "      <td>\\t Je ne le dirai à personne. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36406</th>\n",
       "      <td>They don't know us.</td>\n",
       "      <td>\\t Ils ne nous connaissent pas. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14260</th>\n",
       "      <td>Are you dressed?</td>\n",
       "      <td>\\t Es-tu habillé ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15175</th>\n",
       "      <td>I am in trouble.</td>\n",
       "      <td>\\t Je suis dans le pétrin. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57369</th>\n",
       "      <td>I paid for the damage.</td>\n",
       "      <td>\\t J'ai payé pour les dégâts. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30541</th>\n",
       "      <td>We were all alone.</td>\n",
       "      <td>\\t Nous étions toutes seules. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39354</th>\n",
       "      <td>Do you have a match?</td>\n",
       "      <td>\\t Disposes-tu d'une allumette ? \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                                       tar\n",
       "46333   A squid has ten legs.  \\t Un calamar est muni de dix pattes. \\n\n",
       "9245           That's my dad.                     \\t C'est mon papa. \\n\n",
       "40615    I don't have a clue.     \\t Je n'en ai pas la moindre idée. \\n\n",
       "41639    I won't tell anyone.          \\t Je ne le dirai à personne. \\n\n",
       "36406     They don't know us.        \\t Ils ne nous connaissent pas. \\n\n",
       "14260        Are you dressed?                     \\t Es-tu habillé ? \\n\n",
       "15175        I am in trouble.             \\t Je suis dans le pétrin. \\n\n",
       "57369  I paid for the damage.          \\t J'ai payé pour les dégâts. \\n\n",
       "30541      We were all alone.          \\t Nous étions toutes seules. \\n\n",
       "39354    Do you have a match?       \\t Disposes-tu d'une allumette ? \\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장의 시작과 끝을 구분하기 위해 \\t, \\n을 사용(<sos>, <eos> 대신 사용)\n",
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 char 집합 : 80\n",
      "target 문장의 char 집합 : 104\n"
     ]
    }
   ],
   "source": [
    "# 문자 집합 구축\n",
    "src_vocab = set()\n",
    "for line in lines.src: # 1줄씩 읽음\n",
    "    for char in line: # 1개의 문자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)\n",
    "\n",
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print('source 문장의 char 집합 :', src_vocab_size)\n",
    "print('target 문장의 char 집합 :', tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, 'ï': 77, '’': 78, '€': 79}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'a': 52, 'b': 53, 'c': 54, 'd': 55, 'e': 56, 'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'l': 63, 'm': 64, 'n': 65, 'o': 66, 'p': 67, 'q': 68, 'r': 69, 's': 70, 't': 71, 'u': 72, 'v': 73, 'w': 74, 'x': 75, 'y': 76, 'z': 77, '\\xa0': 78, '«': 79, '»': 80, 'À': 81, 'Ç': 82, 'É': 83, 'Ê': 84, 'Ô': 85, 'à': 86, 'â': 87, 'ç': 88, 'è': 89, 'é': 90, 'ê': 91, 'ë': 92, 'î': 93, 'ï': 94, 'ô': 95, 'ù': 96, 'û': 97, 'œ': 98, '\\u2009': 99, '‘': 100, '’': 101, '\\u202f': 102, '‽': 103}\n"
     ]
    }
   ],
   "source": [
    "# 각 문자에 index 부여\n",
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 정수 인코딩 : [[30, 64, 10], [30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10]]\n"
     ]
    }
   ],
   "source": [
    "# index가 부여된 문자 집합으로부터 encoding\n",
    "# Encoder의 입력으로 사용될 영어 데이터에 대한 정수 인코딩\n",
    "encoder_input = []\n",
    "\n",
    "# 1개의 문장\n",
    "for line in lines.src:\n",
    "    encoded_line = []\n",
    "    # 각 줄에서 1개의 char\n",
    "    for char in line:\n",
    "        # 각 char을 정수로 변환\n",
    "        encoded_line.append(src_to_index[char])\n",
    "    encoder_input.append(encoded_line)\n",
    "print('source 문장의 정수 인코딩 :', encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장의 정수 인코딩 : [[1, 3, 48, 52, 3, 4, 3, 2], [1, 3, 39, 52, 69, 54, 59, 56, 14, 3, 2], [1, 3, 31, 65, 3, 69, 66, 72, 71, 56, 3, 4, 3, 2], [1, 3, 28, 66, 72, 58, 56, 3, 4, 3, 2], [1, 3, 45, 52, 63, 72, 71, 3, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Decoder의 입력으로 사용될 프랑스어 데이터에 대한 정수 인코딩\n",
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    encoded_line = []\n",
    "    for char in line:\n",
    "        encoded_line.append(tar_to_index[char])\n",
    "    decoder_input.append(encoded_line)\n",
    "print('target 문장의 정수 인코딩 :', decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장 레이블의 정수 인코딩 : [[3, 48, 52, 3, 4, 3, 2], [3, 39, 52, 69, 54, 59, 56, 14, 3, 2], [3, 31, 65, 3, 69, 66, 72, 71, 56, 3, 4, 3, 2], [3, 28, 66, 72, 58, 56, 3, 4, 3, 2], [3, 45, 52, 63, 72, 71, 3, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Decoder의 예측값과 비교하기 위한 실제값에서는 시작 심볼이 필요 없으므로 '\\t'를 제거\n",
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    timestep = 0\n",
    "    encoded_line = []\n",
    "    for char in line:\n",
    "        if timestep > 0:\n",
    "            encoded_line.append(tar_to_index[char])\n",
    "        timestep = timestep + 1\n",
    "    decoder_target.append(encoded_line)\n",
    "print('target 문장 레이블의 정수 인코딩 :',decoder_target[:5])\n",
    "# '\\t'에 해당하는 인덱스 1이 제거됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 최대 길이 : 22\n",
      "target 문장의 최대 길이 : 76\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩이 도니 데이터에 대해 padding 수행\n",
    "# 각 언어의 문장의 최대 길이를 사용\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print('source 문장의 최대 길이 :',max_src_len)\n",
    "print('target 문장의 최대 길이 :',max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot-encoding\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teacher Forcing\n",
    "- 훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 하지 않고, `이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값`으로 사용\n",
    "- 이전 시점의 디코더 셀의 예측값이 잘못될 가능성이 높아, 이로 인한 연쇄작용으로 예측에 대한 학습이 잘못되기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "\n",
    "# encoder_outputs은 여기서는 불필요\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 은닉 상태와 셀 상태.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "\n",
    "# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달.\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "750/750 [==============================] - 402s 501ms/step - loss: 0.8512 - val_loss: 0.7731\n",
      "Epoch 2/40\n",
      "750/750 [==============================] - 353s 471ms/step - loss: 0.5713 - val_loss: 0.6639\n",
      "Epoch 3/40\n",
      "750/750 [==============================] - 362s 482ms/step - loss: 0.5016 - val_loss: 0.5990\n",
      "Epoch 4/40\n",
      "750/750 [==============================] - 388s 517ms/step - loss: 0.4567 - val_loss: 0.5514\n",
      "Epoch 5/40\n",
      "750/750 [==============================] - 513s 684ms/step - loss: 0.4223 - val_loss: 0.5232\n",
      "Epoch 6/40\n",
      "750/750 [==============================] - 452s 603ms/step - loss: 0.3956 - val_loss: 0.4907\n",
      "Epoch 7/40\n",
      "750/750 [==============================] - 336s 449ms/step - loss: 0.3752 - val_loss: 0.4736\n",
      "Epoch 8/40\n",
      "750/750 [==============================] - 334s 445ms/step - loss: 0.3583 - val_loss: 0.4539\n",
      "Epoch 9/40\n",
      "750/750 [==============================] - 337s 449ms/step - loss: 0.3442 - val_loss: 0.4425\n",
      "Epoch 10/40\n",
      "750/750 [==============================] - 331s 441ms/step - loss: 0.3322 - val_loss: 0.4325\n",
      "Epoch 11/40\n",
      "750/750 [==============================] - 329s 438ms/step - loss: 0.3222 - val_loss: 0.4196\n",
      "Epoch 12/40\n",
      "750/750 [==============================] - 329s 439ms/step - loss: 0.3131 - val_loss: 0.4111\n",
      "Epoch 13/40\n",
      "750/750 [==============================] - 329s 439ms/step - loss: 0.3052 - val_loss: 0.4049\n",
      "Epoch 14/40\n",
      "750/750 [==============================] - 328s 438ms/step - loss: 0.2980 - val_loss: 0.3977\n",
      "Epoch 15/40\n",
      "750/750 [==============================] - 329s 438ms/step - loss: 0.2915 - val_loss: 0.3924\n",
      "Epoch 16/40\n",
      "750/750 [==============================] - 330s 439ms/step - loss: 0.2853 - val_loss: 0.3865\n",
      "Epoch 17/40\n",
      "750/750 [==============================] - 330s 440ms/step - loss: 0.2799 - val_loss: 0.3809\n",
      "Epoch 18/40\n",
      "750/750 [==============================] - 331s 441ms/step - loss: 0.2747 - val_loss: 0.3773\n",
      "Epoch 19/40\n",
      "750/750 [==============================] - 331s 441ms/step - loss: 0.2700 - val_loss: 0.3767\n",
      "Epoch 20/40\n",
      "750/750 [==============================] - 331s 442ms/step - loss: 0.2655 - val_loss: 0.3701\n",
      "Epoch 21/40\n",
      "750/750 [==============================] - 333s 444ms/step - loss: 0.2612 - val_loss: 0.3666\n",
      "Epoch 22/40\n",
      "750/750 [==============================] - 330s 440ms/step - loss: 0.2570 - val_loss: 0.3643\n",
      "Epoch 23/40\n",
      "750/750 [==============================] - 330s 440ms/step - loss: 0.2533 - val_loss: 0.3619\n",
      "Epoch 24/40\n",
      "750/750 [==============================] - 331s 441ms/step - loss: 0.2497 - val_loss: 0.3591\n",
      "Epoch 25/40\n",
      "750/750 [==============================] - 330s 440ms/step - loss: 0.2462 - val_loss: 0.3571\n",
      "Epoch 26/40\n",
      "750/750 [==============================] - 329s 438ms/step - loss: 0.2430 - val_loss: 0.3581\n",
      "Epoch 27/40\n",
      "750/750 [==============================] - 329s 439ms/step - loss: 0.2398 - val_loss: 0.3540\n",
      "Epoch 28/40\n",
      "750/750 [==============================] - 329s 439ms/step - loss: 0.2369 - val_loss: 0.3547\n",
      "Epoch 29/40\n",
      "750/750 [==============================] - 329s 439ms/step - loss: 0.2339 - val_loss: 0.3509\n",
      "Epoch 30/40\n",
      "750/750 [==============================] - 330s 439ms/step - loss: 0.2311 - val_loss: 0.3515\n",
      "Epoch 31/40\n",
      "750/750 [==============================] - 330s 439ms/step - loss: 0.2285 - val_loss: 0.3494\n",
      "Epoch 32/40\n",
      "750/750 [==============================] - 331s 442ms/step - loss: 0.2260 - val_loss: 0.3480\n",
      "Epoch 33/40\n",
      "750/750 [==============================] - 330s 440ms/step - loss: 0.2235 - val_loss: 0.3474\n",
      "Epoch 34/40\n",
      "750/750 [==============================] - 329s 439ms/step - loss: 0.2210 - val_loss: 0.3469\n",
      "Epoch 35/40\n",
      "750/750 [==============================] - 330s 440ms/step - loss: 0.2188 - val_loss: 0.3447\n",
      "Epoch 36/40\n",
      "750/750 [==============================] - 330s 440ms/step - loss: 0.2165 - val_loss: 0.3457\n",
      "Epoch 37/40\n",
      "750/750 [==============================] - 329s 439ms/step - loss: 0.2143 - val_loss: 0.3446\n",
      "Epoch 38/40\n",
      "750/750 [==============================] - 330s 440ms/step - loss: 0.2123 - val_loss: 0.3455\n",
      "Epoch 39/40\n",
      "750/750 [==============================] - 328s 438ms/step - loss: 0.2101 - val_loss: 0.3441\n",
      "Epoch 40/40\n",
      "750/750 [==============================] - 329s 439ms/step - loss: 0.2082 - val_loss: 0.3459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27a6dc44a90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=40, validation_split=0.2)\n",
    "# Hidden state의 수 256, epochs 40은 overfitting(과적합)이 일어남(val_loss 값이 증가함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\n",
    "# 뒤의 함수 decode_sequence()에 동작을 구현 예정\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태를 버리지 않음.\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "            len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 783ms/step\n",
      "1/1 [==============================] - 1s 754ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장: Bouge ! \n",
      "번역 문장: Sortez ! \n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Hello!\n",
      "정답 문장: Bonjour ! \n",
      "번역 문장: Souvez votre chambre ! \n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Got it!\n",
      "정답 문장: J'ai pigé ! \n",
      "번역 문장: Décampez ! \n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Go home.\n",
      "정답 문장: Rentre à la maison. \n",
      "번역 문장: Allez ! \n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Get going.\n",
      "정답 문장: En avant. \n",
      "번역 문장: Dégage ! \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
